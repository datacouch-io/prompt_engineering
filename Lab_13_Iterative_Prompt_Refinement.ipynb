{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 13: Iterative Prompt Refinement**\n",
        "\n",
        "Welcome to Lab 13! In this lab, we will focus on the art and science of refining prompts to optimize the performance of AI language models. Crafting an effective prompt is an iterative process, where small adjustments can lead to significant improvements in the quality of the generated output. By carefully analyzing model feedback and refining your prompts, you can achieve more accurate, relevant, and efficient results.\n",
        "\n",
        "This lab will cover two key areas:\n",
        "\n",
        "1. **Techniques for Refining Prompts Based on Model Feedback:** Learn how to interpret the outputs provided by the model to iteratively improve your prompts. This involves analyzing how the model responds to different prompts and making adjustments to better align the output with your goals.\n",
        "\n",
        "2. **Reducing Token Usage While Maintaining Output Quality:** Token usage can have a direct impact on both the efficiency and cost of working with language models. We will explore strategies to streamline prompts, reducing the number of tokens used without sacrificing the quality of the generated output.\n",
        "\n",
        "By the end of this lab, you will be equipped with practical techniques to refine your prompts, ensuring that you get the most out of AI language models in an efficient and effective manner.\n",
        "\n",
        "Let’s dive in and start refining!\n"
      ],
      "metadata": {
        "id": "BZKhinRkIP7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Setting Up the Environment**\n",
        "Before we begin evaluating prompts, we need to set up our environment to use OpenAI's GPT models. We'll securely retrieve our API key from environment variables and configure the OpenAI client.\n"
      ],
      "metadata": {
        "id": "B5Ox8cm-IP2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVqE6nk_H6Hh"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = \"your-api-key-here\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Techniques for Refining Prompts Based on Model Feedback**\n",
        "In this step, we will craft prompts and iteratively refine them based on the model's feedback. The goal is to enhance the clarity and relevance of the output by making targeted adjustments to the prompt.\n"
      ],
      "metadata": {
        "id": "UUXiZq4kIZFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_prompt(prompt):\n",
        "    # Call the OpenAI API to generate a response\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # Specify the language model to use\n",
        "        prompt=prompt,              # The initial prompt\n",
        "        max_tokens=100,             # Limit the response length\n",
        "        n=1,                        # Generate a single response\n",
        "        temperature=0.7             # Set temperature for varied creativity\n",
        "    )\n",
        "\n",
        "    # Extract the generated response\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output\n",
        "\n",
        "# Initial prompt\n",
        "initial_prompt = \"Tell me about the benefits of exercise.\"\n",
        "\n",
        "# First refinement: Adding specificity\n",
        "refined_prompt_1 = \"Explain the physical and mental benefits of regular exercise.\"\n",
        "\n",
        "# Second refinement: Direct question\n",
        "refined_prompt_2 = \"What are the top three physical and mental health benefits of regular exercise?\"\n",
        "\n",
        "# Generate and display outputs\n",
        "prompts = [initial_prompt, refined_prompt_1, refined_prompt_2]\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"Prompt {i}: {prompt}\")\n",
        "    output = refine_prompt(prompt)\n",
        "    print(f\"Output: {output}\\n\")\n"
      ],
      "metadata": {
        "id": "D7bs6oYHIdVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Code**\n",
        "- **Refining Prompts:** The `refine_prompt` function generates responses based on the provided prompt, allowing us to iteratively improve the prompt.\n",
        "- **Iterative Refinement:** We start with a general prompt and refine it by adding specificity and rephrasing it as a direct question. This helps the model generate more focused and relevant outputs.\n",
        "- **Output Comparison:** By comparing the outputs generated from each iteration, you can see how refining the prompt leads to clearer and more detailed responses.\n"
      ],
      "metadata": {
        "id": "SWEeAgh-Iq8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Reducing Token Usage While Maintaining Output Quality**\n",
        "In this step, we will explore strategies to reduce token usage in prompts without compromising the quality of the model's output. Efficient prompts help in managing API costs and improving response times.\n"
      ],
      "metadata": {
        "id": "MJ-mswJoItHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_token_usage(prompt):\n",
        "    # Generate a concise response by reducing max tokens\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=50,    # Reduce the max tokens to limit response length\n",
        "        n=1,\n",
        "        temperature=0.5   # Lower temperature for more deterministic output\n",
        "    )\n",
        "\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output\n",
        "\n",
        "# Original verbose prompt\n",
        "verbose_prompt = \"Can you provide an overview of the physical and mental health benefits that regular exercise can offer?\"\n",
        "\n",
        "# Concise prompt with reduced tokens\n",
        "concise_prompt = \"Summarize the health benefits of regular exercise.\"\n",
        "\n",
        "# Generate and display outputs\n",
        "print(\"Original verbose prompt output:\")\n",
        "output_verbose = reduce_token_usage(verbose_prompt)\n",
        "print(output_verbose)\n",
        "\n",
        "print(\"\\nConcise prompt output:\")\n",
        "output_concise = reduce_token_usage(concise_prompt)\n",
        "print(output_concise)\n"
      ],
      "metadata": {
        "id": "1o1g5ubGIu8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Code**\n",
        "- **Token Management:** The `reduce_token_usage` function generates more concise outputs by reducing the number of tokens allowed in the response.\n",
        "- **Concise Prompts:** Shortening the prompt and setting a lower `max_tokens` limit encourages the model to provide more direct and efficient responses.\n",
        "- **Comparing Outputs:** The comparison between the verbose and concise prompts shows how reducing token usage can still yield high-quality responses, often more efficiently.\n"
      ],
      "metadata": {
        "id": "lN4bOD26Ivm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion and Further Exploration**\n",
        "In this lab, you’ve learned how to iteratively refine prompts based on model feedback and strategies for reducing token usage while maintaining output quality. These techniques are essential for optimizing your interactions with AI language models, ensuring both effectiveness and efficiency.\n",
        "\n",
        "To continue honing your skills:\n",
        "- **Experiment with Various Prompts:** Try refining prompts across different topics and observe how each change impacts the quality of the output.\n",
        "- **Optimize for Different Use Cases:** Consider how reducing token usage can benefit different scenarios, such as generating concise summaries or quick answers in chatbot applications.\n",
        "- **Advanced Refinement Techniques:** Explore additional refinement techniques, such as altering the temperature setting or experimenting with different model engines to see how they affect output.\n",
        "\n",
        "Keep practicing these techniques to become a more effective prompt engineer and maximize the potential of AI-driven tools!\n",
        "\n",
        "Happy coding!\n"
      ],
      "metadata": {
        "id": "MJTjzNh6IyLi"
      }
    }
  ]
}