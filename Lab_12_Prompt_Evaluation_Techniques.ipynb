{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 12: Prompt Evaluation Techniques**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Welcome to Lab 12! In this lab, we will explore techniques for evaluating the effectiveness of prompts when working with language models. A well-crafted prompt is essential for guiding the model to produce accurate, relevant, and coherent outputs. However, not all prompts are equally effective, and understanding how to measure and evaluate their effectiveness is key to optimizing your results.\n",
        "\n",
        "We will cover two main areas in this lab:\n",
        "\n",
        "1. **Measuring Prompt Effectiveness:** Learn how to assess the quality of your prompts by evaluating the consistency, relevance, and accuracy of the model's\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "outputs. We will introduce quantitative and qualitative methods for evaluating prompt effectiveness, enabling you to fine-tune prompts for better performance.\n",
        "\n",
        "2. **Analyzing Model Outputs for Accuracy and Relevance:** Once you have generated outputs using different prompts, it is crucial to analyze these outputs to determine how well they meet your objectives. We will look at methods for evaluating the accuracy and relevance of model-generated content, helping you identify areas for improvement.\n",
        "\n",
        "By the end of this lab, you will have a solid understanding of how to evaluate and improve the prompts you use with language models, ensuring that they yield the best possible results.\n",
        "\n",
        "Let's get started!\n"
      ],
      "metadata": {
        "id": "WWVrpxKyGFeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Setting Up the Environment**\n",
        "Before we begin evaluating prompts, we need to set up our environment to use OpenAI's GPT models. We'll securely retrieve our API key from environment variables and configure the OpenAI client.\n"
      ],
      "metadata": {
        "id": "i4Jh0WCJGccu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rElXZu-Cjpn"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = \"your-api-key-here\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Measuring Prompt Effectiveness**\n",
        "In this step, we will craft different prompts and evaluate their effectiveness by analyzing the consistency and relevance of the outputs generated by the model.\n"
      ],
      "metadata": {
        "id": "GA_Udt1xGmPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompt(prompt):\n",
        "    # Call the OpenAI API to generate a response\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # Specify the language model to use\n",
        "        prompt=prompt,              # The prompt to evaluate\n",
        "        max_tokens=150,             # Limit the response length\n",
        "        n=1,                        # Generate a single response\n",
        "        temperature=0.7             # Set temperature for varied creativity\n",
        "    )\n",
        "\n",
        "    # Extract the generated response\n",
        "    output = response.choices[0].text.strip()\n",
        "    return output\n",
        "\n",
        "# Example prompts for evaluation\n",
        "prompts = [\n",
        "    \"Describe the impact of climate change on polar bears.\",\n",
        "    \"What are the effects of climate change?\",\n",
        "    \"Explain the consequences of global warming on Arctic wildlife.\",\n",
        "    \"List the key factors contributing to climate change.\",\n",
        "    \"Climate change consequences?\"\n",
        "]\n",
        "\n",
        "# Evaluate each prompt\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"Prompt {i}: {prompt}\")\n",
        "    output = evaluate_prompt(prompt)\n",
        "    print(f\"Output: {output}\\n\")\n"
      ],
      "metadata": {
        "id": "s1IOeyXzHjld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Code**\n",
        "- **Prompt Evaluation:** The `evaluate_prompt` function generates responses based on the provided prompt. This allows us to compare the outputs and assess the effectiveness of each prompt.\n",
        "- **Temperature Setting:** A moderate temperature of `0.7` introduces some variability in the model's responses, which can be useful for evaluating prompt effectiveness.\n",
        "- **Example Prompts:** We use various prompts on climate change to observe how different wordings and structures affect the model's output.\n",
        "- **Output Analysis:** By reviewing the outputs generated for each prompt, you can assess which prompt leads to the most relevant and detailed responses.\n"
      ],
      "metadata": {
        "id": "jhJ5MWhuHkhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Analyzing Model Outputs for Accuracy and Relevance**\n",
        "In this step, we will analyze the outputs generated from different prompts to determine their accuracy and relevance. This process will help you identify areas where the prompt can be improved.\n"
      ],
      "metadata": {
        "id": "hHmwCJPaHnmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_output(output, keywords):\n",
        "    # Check if the output contains the expected keywords\n",
        "    keyword_hits = [keyword for keyword in keywords if keyword.lower() in output.lower()]\n",
        "    accuracy = len(keyword_hits) / len(keywords)\n",
        "\n",
        "    # Check if the output is relevant to the topic\n",
        "    relevance = all(keyword.lower() in output.lower() for keyword in keywords)\n",
        "\n",
        "    return accuracy, relevance, keyword_hits\n",
        "\n",
        "# Define keywords related to climate change and polar bears\n",
        "keywords = [\"climate change\", \"polar bears\", \"habitat\", \"ice\", \"food\", \"warming\"]\n",
        "\n",
        "# Analyze outputs for relevance and accuracy\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    output = evaluate_prompt(prompt)\n",
        "    accuracy, relevance, keyword_hits = analyze_output(output, keywords)\n",
        "\n",
        "    print(f\"Prompt {i} Analysis:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")  # Show accuracy as a percentage\n",
        "    print(f\"Relevance: {'Yes' if relevance else 'No'}\")  # Check if all keywords are present\n",
        "    print(f\"Keywords Found: {', '.join(keyword_hits)}\\n\")  # List the keywords found in the output\n"
      ],
      "metadata": {
        "id": "2NC231onHq5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of the Code**\n",
        "- **Output Analysis:** The `analyze_output` function checks the accuracy and relevance of the model's output by looking for specific keywords related to the topic.\n",
        "- **Accuracy Metric:** Accuracy is calculated as the proportion of keywords found in the output compared to the total number of expected keywords.\n",
        "- **Relevance Check:** Relevance is determined by whether all the expected keywords are present in the output.\n",
        "- **Keyword Hits:** The specific keywords found in the output are listed, helping you understand how well the model addressed the prompt.\n",
        "- **Expanded Analysis:** We analyze the outputs generated for various prompts about climate change and polar bears, assessing their accuracy and relevance based on an expanded set of keywords.\n"
      ],
      "metadata": {
        "id": "WcejkDSDHrsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion and Further Exploration**\n",
        "In this lab, you’ve learned how to measure the effectiveness of prompts and analyze the outputs of language models for accuracy and relevance. These skills are essential for optimizing the performance of AI-driven content generation and ensuring that the outputs meet your specific needs.\n",
        "\n",
        "To further enhance your understanding:\n",
        "- **Experiment with Different Prompts:** Try crafting prompts for different topics and evaluate how effectively they guide the model’s outputs.\n",
        "- **Advanced Evaluation Techniques:** Explore more sophisticated evaluation techniques, such as using BLEU scores or ROUGE metrics to assess output quality.\n",
        "- **Real-World Applications:** Consider applying these techniques to real-world scenarios, such as generating content for marketing, education, or research.\n",
        "\n",
        "Keep refining your prompt engineering skills to unlock the full potential of AI-powered language models!\n",
        "\n",
        "Happy coding!\n"
      ],
      "metadata": {
        "id": "XOMieedBHtza"
      }
    }
  ]
}