{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9d191f-5d63-499d-8095-1795cadab9e8",
   "metadata": {},
   "source": [
    "# Prompt Engineering Game Quest\n",
    "\n",
    "Welcome to the Prompt Engineering game quest. This game quest is designed to teach prompt engineering best practices by solving quest questions. This notebook begins by introducing various prompting concepts that you will use later to solve the quest questions.\n",
    "\n",
    "**Notebook Configuration**  \n",
    "Inference parameters are located in  `src/completions.py`, including:\n",
    "\n",
    "-   OpenAI API Base URL (Default:  `https://api.openai.com/v1/engines/`)\n",
    "-   OpenAI Model (Default:  `gpt-4`)\n",
    "-   Temperature (Default: 0)\n",
    "-   Other inference parameters such as:  `top_k`,  `top_p`,  `max_tokens`,  `stop_sequences`, etc.\n",
    "\n",
    "**Note:**  Code sections might fail if you haven't enabled API access or if your API key is not correctly configured. Ensure that you have set up your  [OpenAI API key](https://platform.openai.com/account/api-keys)  and configured the API Base URL according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dc407",
   "metadata": {},
   "source": [
    "## Model Providers Used in This Game Quest\n",
    "\n",
    "The models used in this game quest are sourced from OpenAI, specifically the GPT-4 model, accessible via the OpenAI API.\n",
    "\n",
    "**OpenAI**  OpenAI's GPT-4 model is a state-of-the-art language model that offers a blend of intelligence, versatility, and performance. GPT-4 is designed to handle a wide array of tasks, from natural language understanding and generation to more complex reasoning and problem-solving. With its advanced capabilities, GPT-4 can process text data to generate human-like responses, making it ideal for a variety of applications across different domains. The model is continually updated and optimized to meet the growing demands for AI systems that can understand and generate text in a highly coherent and contextually relevant manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f0e74",
   "metadata": {},
   "source": [
    "**OpenAI**  \n",
    "OpenAI is a leading AI research and deployment company that develops cutting-edge AI models, including the GPT-4 model used in this game quest. OpenAI models are designed to be efficient, helpful, and reliable, consistently delivering high-quality results across various tasks. With a focus on innovation and ethical AI development, OpenAI continually enhances its models to meet the highest standards of performance and trustworthiness.\n",
    "\n",
    "Let's begin by installing the required Python packages and verifying model access with a simple prompt invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc746d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install -r requirements.txt --quiet\n",
    "\n",
    "# Restart the kernel if you are facing erros such as: \n",
    "# shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
    "# The folder you are executing pip from can no longer be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c0dc37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-4:\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Default OpenAI model: gpt-4\n",
    "\n",
    "from src.completions import get_openai_completion\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = \"\"\n",
    "\n",
    "PROMPT = \"Hello, GPT-4!\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT}\n",
    "]\n",
    "\n",
    "print(f'\\nGPT-4:\\n{get_openai_completion(messages)}')\n",
    "\n",
    "# Restart the kernel if you face errors such as:\n",
    "# ModuleNotFoundError: No module named 'src'\n",
    "\n",
    "# Modify your OpenAI API key and endpoint configuration within src/completions.py, \n",
    "# and restart the kernel if you encounter issues such as:\n",
    "# InvalidRequestError: Invalid request, or\n",
    "# AuthenticationError: Incorrect API key provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789fc8f-03c5-4b22-a527-f63354249e39",
   "metadata": {},
   "source": [
    "# Prompt Engineering Game Quest\n",
    "\n",
    "Welcome to the Prompt Engineering game quest. This game quest is designed to teach prompt engineering best practices by solving quest questions. This notebook begins by introducing various prompting concepts that you will use later to solve the quest questions.\n",
    "\n",
    "**Notebook Configuration**  \n",
    "Inference parameters are located in  `src/completions.py`, including:\n",
    "\n",
    "-   OpenAI API Base URL (Default:  `https://api.openai.com/v1/engines/`)\n",
    "-   OpenAI Model (Default:  `gpt-4`)\n",
    "-   Temperature (Default: 0)\n",
    "-   Other inference parameters such as:  `top_k`,  `top_p`,  `max_tokens`,  `stop_sequences`, etc.\n",
    "\n",
    "**Note:**  Code sections might fail if you haven't enabled API access or if your API key is not correctly configured. Ensure that you have set up your  [OpenAI API key](https://platform.openai.com/account/api-keys)  and configured the API Base URL according to your needs.\n",
    "\n",
    "## Model Providers Used in This Game Quest\n",
    "\n",
    "The models used in this game quest are sourced from OpenAI, specifically the GPT-4 model, accessible via the OpenAI API.\n",
    "\n",
    "**OpenAI**  OpenAI's GPT-4 model is a state-of-the-art language model that offers a blend of intelligence, versatility, and performance. GPT-4 is designed to handle a wide array of tasks, from natural language understanding and generation to more complex reasoning and problem-solving. With its advanced capabilities, GPT-4 can process text data to generate human-like responses, making it ideal for a variety of applications across different domains. The model is continually updated and optimized to meet the growing demands for AI systems that can understand and generate text in a highly coherent and contextually relevant manner.\n",
    "\n",
    "**OpenAI**  \n",
    "OpenAI is a leading AI research and deployment company that develops cutting-edge AI models, including the GPT-4 model used in this game quest. OpenAI models are designed to be efficient, helpful, and reliable, consistently delivering high-quality results across various tasks. With a focus on innovation and ethical AI development, OpenAI continually enhances its models to meet the highest standards of performance and trustworthiness.\n",
    "\n",
    "Let's begin by installing the required Python packages and verifying model access with a simple prompt invocation.\n",
    "\n",
    "\n",
    "## Prompt Structures\n",
    "\n",
    "Prompt engineering involves crafting and refining text prompts to guide large language models (LLMs), ensuring the model's output aligns with the user's intent and requirements. Effective prompt engineering usually contains one or more conceptual prompt components that help direct the model's attention in the correct direction. These components help articulate the prompt designer's intent, allowing LLMs to tackle complex tasks, generate high-quality content, and offer valuable insights. Prompts that lack certain components can result in long, inefficient conversations with many query refinements.\n",
    "\n",
    "The general prompt structure used among most LLMs is called the COSTAR framework, though each LLM might have its own refinement for this general structure.\n",
    "\n",
    "### COSTAR Methodology\n",
    "\n",
    "COSTAR is a structured methodology that guides you through the process of crafting effective prompts for LLMs. By following its step-by-step approach, you can design prompts that are tailored to generate the types of responses you need from the LLM. The elegance of COSTAR lies in its versatility – it provides a robust foundation for prompt engineering, regardless of the specific technique or approach you employ. Whether you're leveraging few-shot learning, chain-of-thought prompting, or any other method (covered later in other sections), the COSTAR framework equips you with a systematic way to formulate prompts that unlock the full potential of LLMs.\n",
    "\n",
    "By breaking down the prompt creation process into distinct stages, COSTAR empowers you to methodically refine and optimize your prompts, ensuring that every aspect is carefully considered and aligned with your goals. This level of rigor and deliberation ultimately translates into more accurate, coherent, and valuable outputs from the language model.\n",
    "\n",
    "**Context (C)**  \n",
    "Providing background information helps the LLM understand the specific scenario, ensuring relevance in its responses.\n",
    "\n",
    "Example: I am a personal productivity developer. In the realm of personal development and productivity, there is a growing demand for systems that not only help individuals set goals but also convert those goals into actionable steps. Many struggle with the transition from aspirations to concrete actions, highlighting the need for an effective goal-to-system conversion process.\n",
    "\n",
    "**Objective (O)**  \n",
    "Clearly defining the task directs the LLM’s focus to meet that specific goal.\n",
    "\n",
    "Example: Your task is to guide me in creating a comprehensive system converter. This involves breaking down the process into distinct steps, including identifying the goal, employing the 5 Whys technique, learning core actions, setting intentions, and conducting periodic reviews. The aim is to provide a step-by-step guide for seamlessly transforming goals into actionable plans.\n",
    "\n",
    "**Style (S)**  \n",
    "Specifying the desired writing style, such as emulating a famous personality or professional expert, guides the LLM to align its response with your needs.\n",
    "\n",
    "Example: Write in an informative and instructional style, resembling a guide on personal development. Ensure clarity and coherence in the presentation of each step, catering to an audience keen on enhancing their productivity and goal attainment skills.\n",
    "\n",
    "**Tone (T)**  \n",
    "Setting the tone ensures the response resonates with the required sentiment, whether it be formal, humorous, or empathetic.\n",
    "\n",
    "Example: Maintain a positive and motivational tone throughout, fostering a sense of empowerment and encouragement. It should feel like a friendly guide offering valuable insights.\n",
    "\n",
    "**Audience (A)**  \n",
    "Identifying the intended audience tailors the LLM’s response to be appropriate and understandable for specific groups, such as experts or beginners.\n",
    "\n",
    "Example: The target audience is individuals interested in personal development and productivity enhancement. Assume a readership that seeks practical advice and actionable steps to turn their goals into tangible outcomes.\n",
    "\n",
    "**Response (R)**  \n",
    "Providing the response format, like a list or JSON, ensures the LLM outputs in the required structure for downstream tasks.\n",
    "\n",
    "Example: Provide a structured list of steps for the goal-to-system conversion process. Each step should be clearly defined, and the overall format should be easy to follow for quick implementation.\n",
    "\n",
    "### GPT-4 Prompt Basics\n",
    "\n",
    "  \n",
    "**Figure 2**: GPT-4 Recommended Prompt Structure\n",
    "\n",
    "**Use Clear and Structured Prompts**  \n",
    "GPT-4 performs best with prompts that are clear and structured. Although it doesn't require specific tags like XML, it benefits from well-organized inputs that delineate different sections of the prompt. This approach helps GPT-4 understand the context and generate more accurate outputs, especially when working with complex or multi-step tasks.\n",
    "\n",
    "Using clear sections in your prompts can improve GPT-4's accuracy and help it maintain coherence across long responses, making it easier to extract key information programmatically.\n",
    "\n",
    "For more on crafting effective prompts, see  [OpenAI's Prompt Engineering Guide](https://platform.openai.com/docs/guides/gpt)  and  [OpenAI's Best Practices](https://platform.openai.com/docs/best-practices).\n",
    "\n",
    "**Messages API**  \n",
    "The OpenAI API uses a \"messages\" format for interactions, where each message in the conversation is an object with a \"role\" (user or assistant) and \"content\" (the text of the message). GPT-4 models are designed to handle conversations by alternating between user inputs and assistant responses.\n",
    "\n",
    "The Messages API expects input in a JSON format where the main object is an array of message objects. Each message object should have the following structure:\n",
    "```\n",
    "{\n",
    "  \"role\": \"user|assistant\",\n",
    "  \"content\": \"The actual text content of the message\"\n",
    "} \n",
    "```\n",
    "When the role is set to \"assistant,\" \"content\" represents GPT-4's response.\n",
    "\n",
    "A simple example of the \"messages\" array with one message from a user could be:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello, how are you?\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "GPT-4 models are trained to operate on alternating user and assistant conversational turns. You can include multiple message objects in the \"messages\" array to represent a full conversation history. GPT-4 will then generate a response based on the entire context provided.\n",
    "\n",
    "`user`  and  `assistant`  messages  **MUST alternate**, and messages  **MUST start**  with a  `user`  turn. You can have multiple user & assistant pairs in a prompt (as if simulating a multi-turn conversation).\n",
    "\n",
    "For more details on the API format, check out the  [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "Here is an example of a chat:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the capital of France?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Recommend some places to visit in Paris.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Multimodal Prompts**  \n",
    "While GPT-4 primarily processes text, OpenAI has introduced multimodal models that can handle both text and image inputs. In these cases, you specify the modalities in the content input field, with images sent to the model in base64 format. However, GPT-4 does not yet support multiple modalities within the same API call, and images cannot be used in the \"assistant\" role.\n",
    "\n",
    "For more information on multimodal models, you can refer to  [OpenAI's GPT-4 Vision Documentation](https://platform.openai.com/docs/models/gpt-4).\n",
    "\n",
    "Example of a text and image input with a hypothetical multimodal GPT-4:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"model\": \"gpt-4-multimodal\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image\",\n",
    "          \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": \"image/jpeg\",\n",
    "            \"data\": \"iVBORw...\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What's in this image?\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "``` \n",
    "\n",
    "**Prefill GPT-4’s Response**  \n",
    "When using GPT-4, you can guide its responses and control the output format by prefilling the  `assistant`  content and providing clear instructions. These techniques allow you to direct GPT-4's actions, specify the structure and style of the generated content, and even help GPT-4 stay on track during complex tasks. By leveraging prefilling and output format control, you can significantly improve GPT-4's performance and obtain more accurate and tailored responses.\n",
    "\n",
    "For example, to guide GPT-4 to output in JSON format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"List some popular tourist attractions in Paris in JSON format.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"{\\n  \\\"attractions\\\": [\\n    \\\"Eiffel Tower\\\",\\n    \\\"Louvre Museum\\\",\\n    \\\"Notre-Dame Cathedral\\\"\\n  ]\\n}\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "``` \n",
    "\n",
    "In this example, GPT-4 is instructed to output its response in JSON format, which it follows based on the prefilled assistant content.\n",
    "\n",
    "For more detailed guidance on advanced prompt techniques, visit  [OpenAI's Advanced Usage Guide](https://platform.openai.com/docs/guides/advanced-usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef171047-d05b-4b03-8be6-b00906ae5409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: SmartHome Mini\n",
      "Size: 5 inches wide\n",
      "Price: $49.99\n",
      "Color: Black or White\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Assuming get_openai_completion is a function that wraps the OpenAI API call\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Please extract the name, size, price, and color from this product description.\n",
    "    <description>\n",
    "        The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99.\n",
    "        At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice \n",
    "        or app—no matter where you place it in your home.\n",
    "        This affordable little hub brings convenient hands-free control to your smart devices.\n",
    "    </description>\n",
    "\"\"\"\n",
    "\n",
    "# We add 'assistant' role with an opening '{' to hint GPT-4 at the desired output format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"{\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the completion function and print the result\n",
    "response = get_openai_completion(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94dd9d",
   "metadata": {},
   "source": [
    "### What is a System Prompt?\n",
    "\n",
    "A  **system prompt**  in the context of OpenAI's GPT-4 is a powerful tool used to provide initial context, scope, examples, guardrails, or desired output formats to the model before presenting it with a specific question or task. The system prompt serves as an additional layer of guidance and control over the model's output, helping to ensure that GPT-4's responses align with the specific goals or tasks at hand.\n",
    "\n",
    "System prompts are particularly useful for setting the tone, style, and overall behavior of the AI during interactions. By specifying the global context or rules that apply to all requests during a conversation, system prompts can help the model maintain consistency across its responses. Any information that is unique to a particular request should be included in the user role within the conversation structure.\n",
    "\n",
    "While system prompts can increase GPT-4's robustness and resilience against unwanted behavior or outputs, they do not guarantee complete protection against potential issues like jailbreaks or unintended outputs.\n",
    "\n",
    "System prompts will play a key role in our challenges, as they help guide the AI to deliver responses that are accurate, coherent, and aligned with the specific objectives of each task.\n",
    "\n",
    "For more information on system prompts and their usage with GPT-4, refer to  [OpenAI's documentation on system prompts](https://platform.openai.com/docs/guides/gpt/system-prompts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2056bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in the heart of the roarin' seven seas, there be a notorious pirate named One-Eyed Jack. He be the most feared pirate across all the oceans, known far and wide for his ruthless tactics and insatiable lust for treasure.\n",
      "\n",
      "One day, One-Eyed Jack got wind of a treasure map leadin' to the elusive Pearl of Poseidon, a gem rumored to be worth ten royal galleons. With a hearty laugh and a glint in his remaining eye, Jack set sail, his sights set on this priceless treasure.\n",
      "\n",
      "The journey was filled with many a peril: monstrous sea creatures, rival pirate crews, and the treacherous weather of the open sea. But One-Eyed Jack and his trusty crew persevered, their hearts filled with the promise of untold riches.\n",
      "\n",
      "After many a day and night, they arrived at the location marked on the map, a deserted island shaped like a skull. With shovels in hand, they dug and dug until their hands bled, and finally, their efforts paid off. They unearthed a chest, ornate and glistening in the moonlight.\n",
      "\n",
      "With a hearty cheer, One-Eyed Jack pried open the chest, expecting to see the Pearl of Poseidon. But alas, the chest was empty, save for a single piece of parchment. On it, written in the finest calligraphy, was a single line: \"The greatest treasure be the adventure itself.\"\n",
      "\n",
      "One-Eyed Jack stared at the parchment, then burst out laughing. He held the parchment high, his laughter echoing across the silent ocean. His crew joined in, their laughter a testament to their shared camaraderie and the thrill of their adventure.\n",
      "\n",
      "From that day forth, One-Eyed Jack and his crew sailed the seas, not in search of gold or gems, but for the thrill of the adventure, the camaraderie of their crew, and the endless stories the sea had to offer. Arrr, 'twas the life of a true pirate.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Assuming get_openai_completion is a function that wraps the OpenAI API call\n",
    "def get_openai_completion(messages, system=None):\n",
    "    if system:\n",
    "        # Include the system message if provided\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system})\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Comment out one of the below system prompts that you wish to try out as your character for the story\n",
    "system = \"All your output must be pirate speech\"\n",
    "#system = \"All your output must be real English words without the letter 'a'\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a very short story\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the completion function and print the result\n",
    "response = get_openai_completion(messages=messages, system=system)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb17ece",
   "metadata": {},
   "source": [
    "### OpenAI GPT-4 Prompt Resources\n",
    "\n",
    "-   [OpenAI's Prompt Engineering Guide](https://platform.openai.com/docs/guides/gpt)\n",
    "-   [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "-   [OpenAI's Best Practices for Prompting](https://platform.openai.com/docs/best-practices)\n",
    "-   [OpenAI's Advanced Prompt Engineering Techniques](https://platform.openai.com/docs/guides/advanced-usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f8ef",
   "metadata": {},
   "source": [
    "### OpenAI Recommended Prompt Structure\n",
    "\n",
    "OpenAI's GPT-4 model, like other advanced language models, provides a strong foundation with broad language understanding and performance across various tasks. To get the most out of GPT-4, it is important to format prompts effectively. While GPT-4 does not require specific tokens or special formatting like Mistral, certain prompt structures and patterns can greatly enhance the quality of the model's output.\n",
    "\n",
    "#### GPT-4 Prompt Basics\n",
    "\n",
    "Unlike Mistral's structured prompt format, GPT-4 uses a more flexible approach where the context, instructions, and desired output format are clearly defined within the prompt itself. Here are some key strategies:\n",
    "\n",
    "1.  **Context Setting**: Start your prompt by providing necessary background information or context that helps GPT-4 understand the scenario or task.\n",
    "    \n",
    "    -   Example: \"You are a historian specializing in ancient Rome.\"\n",
    "2.  **Clear Instruction**: Provide explicit instructions about what you want GPT-4 to do.\n",
    "    \n",
    "    -   Example: \"Please write a short essay explaining the significance of Julius Caesar in Roman history.\"\n",
    "3.  **Output Format Guidance**: If you need the output in a specific format, make that clear in your prompt.\n",
    "    \n",
    "    -   Example: \"Provide the answer in a bulleted list.\"\n",
    "4.  **Role-Play or Persona**: You can instruct GPT-4 to assume a specific role or persona, similar to Mistral's character-based prompts.\n",
    "    \n",
    "    -   Example: \"Speak as if you are a pirate telling a story about buried treasure.\"\n",
    "\n",
    "Here is how you can format a prompt for GPT-4:\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the process of photosynthesis in simple terms.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_openai_completion(messages)\n",
    "print(response)` \n",
    "```\n",
    "\n",
    "#### Multi-Turn Conversations\n",
    "\n",
    "For chat-based interactions, you can structure the conversation to include multiple turns. OpenAI's API supports a  `messages`  array where the conversation alternates between the  `user`  and  `assistant`  roles:\n",
    "\n",
    "```\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a knowledgeable guide.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are the best practices for prompt engineering?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Prompt engineering is the process of crafting inputs to guide the AI to produce desired outputs...\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you give an example?\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "In this case, the conversation continues based on the context provided, with GPT-4 generating responses that maintain coherence and relevance throughout the interaction.\n",
    "\n",
    "#### Tokenization in OpenAI\n",
    "\n",
    "OpenAI models, like GPT-4, also rely on tokenization to process text, although the specific tokens like  `<s>`  (beginning of sequence) and  `</s>`  (end of sequence) are handled internally and not explicitly required in prompts:\n",
    "\n",
    "1.  **Tokens**: The input text is split into smaller pieces called tokens, which the model processes to generate responses. Tokenization helps the model manage text efficiently and generate accurate outputs.\n",
    "2.  **Sequence Tokens**: While GPT-4 handles the beginning and end of sequences automatically, understanding token limits (e.g., the maximum number of tokens) is important when crafting longer prompts or managing multi-turn conversations.\n",
    "\n",
    "For more detailed guidance on how OpenAI handles tokenization, refer to  [OpenAI's Tokenization Documentation](https://platform.openai.com/docs/guides/gpt/tokenization).\n",
    "\n",
    "Though GPT-4 doesn't require the same strict prompt format as Mistral's Instruct models, the general principles of providing clear, concise, and context-rich prompts apply universally across different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0da571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import openai\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate user/assistant/user/assistant/...\"\"\"\n",
    "    messages = []\n",
    "    for message in instructions:\n",
    "        messages.append({\"role\": message[\"role\"], \"content\": message[\"content\"].strip()})\n",
    "    return messages\n",
    "\n",
    "def print_instructions(prompt: List[Dict[str, str]], response: str) -> None:\n",
    "    bold, unbold = '\\033[1m', '\\033[0m'\n",
    "    print(f\"{bold}> Input{unbold}\")\n",
    "    for message in prompt:\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "    print(f\"\\n{bold}> Output{unbold}\\n{response}\\n\")\n",
    "\n",
    "def get_openai_completion(messages: List[Dict[str, str]]) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570c844a-21ec-432a-9ea1-2d416f472500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Input\u001b[0m\n",
      "User: Generate 4 very short sentences.\n",
      "\n",
      "\u001b[1m> Output\u001b[0m\n",
      "1. \"The sun set beautifully.\"\n",
      "2. \"My cat purred softly.\"\n",
      "3. \"She baked a big cake.\"\n",
      "4. \"He won the race.\"\n",
      "\n",
      "\u001b[1m> Input\u001b[0m\n",
      "User: Generate 4 very short sentences.\n",
      "Assistant: 1. \"The sun set beautifully.\"\n",
      "2. \"My cat purred softly.\"\n",
      "3. \"She baked a big cake.\"\n",
      "4. \"He won the race.\"\n",
      "User: Categorize each sentence as positive, negative, or neutral.\n",
      "\n",
      "\u001b[1m> Output\u001b[0m\n",
      "1. \"The sun set beautifully.\" - Neutral\n",
      "2. \"My cat purred softly.\" - Positive\n",
      "3. \"She baked a big cake.\" - Positive\n",
      "4. \"He won the race.\" - Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat example\n",
    "instruction1 = \"Generate 4 very short sentences.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruction1}\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_openai_completion(prompt)\n",
    "print_instructions(prompt, response)\n",
    "\n",
    "instruction2 = \"Categorize each sentence as positive, negative, or neutral.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruction1},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "    {\"role\": \"user\", \"content\": instruction2}\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_openai_completion(prompt)\n",
    "print_instructions(prompt, response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773741f3",
   "metadata": {},
   "source": [
    "\n",
    "### Delimiters\n",
    "\n",
    "Delimiters are a crucial tool in prompt engineering, used to clearly specify the boundaries between different sections of text within a prompt. They help to improve the quality and accuracy of the model's responses by ensuring that the model correctly interprets which parts of the input are instructions, which are the actual data to be processed, and which are other relevant sections.\n",
    "\n",
    "**What are Delimiters?**  Delimiters can be any distinct symbols or characters that set apart different sections of text. Common examples include:\n",
    "\n",
    "-   Triple quotes:  `\"\"\"`\n",
    "-   Angle brackets:  `< >`\n",
    "-   Colons:  `:`\n",
    "-   Hashes:  `###`\n",
    "-   Custom tags:  `<<< >>>`\n",
    "\n",
    "**Benefits of Using Delimiters:**\n",
    "\n",
    "1.  **Improved Clarity:**  Delimiters help the model understand the structure of the prompt, clearly distinguishing between instructions and input data. This clarity leads to more accurate and contextually appropriate responses.\n",
    "    \n",
    "2.  **Enhanced Security:**  By using delimiters, you can prevent prompt injection attacks. Prompt injection is when unintended commands or instructions are injected into the prompt, potentially altering the behavior of the model. Delimiters help mitigate this risk by ensuring that anything within the delimiters is treated strictly as input data and not as additional instructions.\n",
    "    \n",
    "3.  **Response Precision:**  Delimiters make it easier for the model to process the prompt correctly, leading to more precise and relevant outputs.\n",
    "    \n",
    "\n",
    "**Example Usage:**\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "prompt = \"\"\"\n",
    "Please summarize the following text:\n",
    "\n",
    "<<<\n",
    "The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app—no matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices.\n",
    ">>>\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content']) \n",
    "```\n",
    "In this example:\n",
    "\n",
    "-   The delimiters  `<<< >>>`  clearly separate the text to be summarized from the rest of the prompt.\n",
    "-   This separation helps the model accurately interpret what needs to be summarized without confusing the input text with additional instructions.\n",
    "\n",
    "**Why Use Delimiters?**  Delimiters are especially useful when handling complex prompts that include multiple components, such as instructions, user input, and examples. By clearly delineating each part of the prompt, you reduce the likelihood of misinterpretation by the model and increase the overall effectiveness and safety of the prompt.\n",
    "\n",
    "For more detailed guidance on using delimiters and other prompt structuring techniques, refer to  [OpenAI's Prompt Engineering Documentation](https://platform.openai.com/docs/guides/gpt/prompt-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e20506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change pin\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define the inquiry and prompt template\n",
    "inquiry = \"I forgot my card security number\"\n",
    "prompt_template = f\"\"\"\n",
    "You are a bank customer service bot. Your task is to assess customer intent \n",
    "and categorize customer inquiry after <<<>>> into one of the following predefined categories:\n",
    "\n",
    "card arrival\n",
    "change pin\n",
    "exchange rate\n",
    "country support \n",
    "cancel transfer\n",
    "charge dispute\n",
    "\n",
    "If the text doesn't fit into any of the above categories, classify it as:\n",
    "customer service\n",
    "\n",
    "You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes. \n",
    "\n",
    "####\n",
    "Here are some examples:\n",
    "\n",
    "Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "Category: card arrival\n",
    "Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "Category: exchange rate \n",
    "Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "Category: country support\n",
    "Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "Category: customer service\n",
    "###\n",
    "\n",
    "<<<\n",
    "Inquiry: {inquiry}\n",
    ">>>\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the messages for the API call\n",
    "messages = [\n",
    "  { \"role\": \"user\", \"content\": prompt_template }\n",
    "]\n",
    "\n",
    "# Function to call GPT-4 and get the completion\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7  # Adjust temperature as needed\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Get the response from GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e80de7",
   "metadata": {},
   "source": [
    "### OpenAI GPT-4 Prompt Resources\n",
    "\n",
    "-   [OpenAI's Guide to Prompting](https://platform.openai.com/docs/guides/gpt)  \n",
    "    This guide covers best practices and techniques for creating effective prompts that generate accurate and useful outputs from OpenAI models.\n",
    "    \n",
    "-   [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)  \n",
    "    Detailed documentation for using OpenAI's API, including how to structure requests, manage outputs, and optimize model performance.\n",
    "    \n",
    "-   [Advanced Prompt Engineering Techniques](https://platform.openai.com/docs/guides/advanced-usage)  \n",
    "    A deep dive into more sophisticated methods of prompt engineering, including multi-turn conversations, few-shot learning, and handling complex scenarios.\n",
    "    \n",
    "-   [Tokenization in OpenAI Models](https://platform.openai.com/docs/guides/gpt/tokenization)  \n",
    "    Learn how OpenAI's models process text through tokenization, which is crucial for understanding model behavior and optimizing prompt design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6493fbf-5b8f-4248-a74d-8de2bcc91b25",
   "metadata": {},
   "source": [
    "## Inference Parameters for GPT-4 Models\n",
    "\n",
    "Inference parameters allow you to influence the responses generated by OpenAI's GPT-4 model. These parameters can be adjusted to fine-tune the model's output based on the specific requirements of your use case. Below is a simplified explanation of the key inference parameters you can control:\n",
    "\n",
    "### Key Inference Parameters\n",
    "\n",
    "**`max_tokens`**  - Specifies the maximum number of tokens the model can generate in response to a given prompt. The model might stop generating tokens before reaching this limit if it determines the response is complete. Setting  `max_tokens`appropriately can help manage both performance and cost:\n",
    "\n",
    "-   **Performance Optimization**: By setting a reasonable  `max_tokens`  limit, you can prevent the model from generating excessively long responses, which can save time and computational resources.\n",
    "-   **Cost Management**: OpenAI charges based on the number of tokens processed, including both input and output tokens. Setting a  `max_tokens`  limit helps control costs by capping the maximum number of tokens the model will generate.\n",
    "\n",
    "**`temperature`**  - Controls the creativity and randomness of the model's output. A lower value (e.g., 0.2) makes the model's output more deterministic and focused, producing similar results for the same prompt. A higher value (e.g., 0.8) increases the randomness and diversity of responses, allowing for more creative or varied outputs.\n",
    "\n",
    "![Temperature](assets/temperature.png)\n",
    "\n",
    "**`top_p`  (nucleus sampling)**  - Determines the diversity of the model's output by considering only the smallest possible set of tokens whose cumulative probability exceeds the top_p value. For example, a  `top_p`  of 0.9 means the model will consider only the tokens that together make up 90% of the probability distribution for the next token, which helps to balance diversity and quality.\n",
    "\n",
    "**`frequency_penalty`**  - A value between -2.0 and 2.0 that influences the likelihood of the model repeating the same line of text. A positive value decreases the model's likelihood of repeating the same line, encouraging more varied output.\n",
    "\n",
    "**`presence_penalty`**  - A value between -2.0 and 2.0 that affects the model's tendency to introduce new topics or ideas. A higher value increases the likelihood of the model mentioning new concepts, while a lower value makes it stick closer to the prompt's context.\n",
    "\n",
    "**`stop`**  - Custom text sequences that instruct the model to stop generating further tokens once a specified sequence is generated. This is useful for preventing the model from producing unwanted text or for structuring the output more precisely.\n",
    "\n",
    "### Example Usage in OpenAI API\n",
    "\n",
    "Here is an example of how you might set these parameters when making a call to OpenAI's API:\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about the ocean.\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.0,\n",
    "    stop=[\"\\n\\n\"]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n",
    "```\n",
    "\n",
    "### Notes:\n",
    "\n",
    "-   **Parameter Tuning**: Feel free to experiment with these parameters during your development process to see how they influence the model's behavior and output.\n",
    "-   **Documentation**: For detailed descriptions and examples of these parameters, refer to  [OpenAI's API Documentation](https://platform.openai.com/docs/api-reference).\n",
    "\n",
    "By adjusting these parameters, you can tailor GPT-4's responses to better fit your specific needs, balancing creativity, coherence, and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8185c",
   "metadata": {},
   "source": [
    "## Basic Prompting Concepts\n",
    "\n",
    "### Zero-shot Prompting\n",
    "\n",
    "Zero-shot prompting refers to a method where the prompt relies solely on the model's general knowledge without providing any examples of how the task should be done. The model uses its extensive pre-training on diverse datasets to infer how to approach and solve the task based on the prompt provided.\n",
    "\n",
    "**Recommended Uses:**\n",
    "\n",
    "-   For tasks with longer inputs and outputs where providing examples could be cost-prohibitive or slow down response times.\n",
    "-   When the output structure is flexible or can be reliably generated without needing specific examples.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> \"I thought it was pretty decent.\"  \n",
    "> What is the sentiment of the above text? Respond with either \"Positive\" or \"Negative.\"\n",
    "\n",
    "### Few-shot Prompting\n",
    "\n",
    "Few-shot prompting involves providing the model with a few examples of a specific task within the prompt. These examples help guide the model on how the task should be performed, improving the model's ability to generate accurate and relevant responses. Few-shot prompting is particularly useful when precise output structure is required.\n",
    "\n",
    "**Recommended Uses:**\n",
    "\n",
    "-   When tasks have shorter inputs or outputs, to minimize token consumption.\n",
    "-   When a precise output structure is necessary, and zero-shot prompting might not provide reliable results.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> \"I liked it\" // Positive  \n",
    "> \"It was OK\" // Positive  \n",
    "> \"Couldn't believe how bad it was!\" // Negative  \n",
    "> \"Not my favorite\" // Negative  \n",
    "> \"This is meh.\"  \n",
    "> What is the sentiment of the above text?\n",
    "\n",
    "### Chain of Thought Prompting (CoT)\n",
    "\n",
    "[Chain-of-thought (CoT) prompting](https://arxiv.org/abs/2201.11903)  enhances the reasoning abilities of language models by breaking down complex questions or tasks into smaller, more manageable steps. This method mimics human reasoning by systematically decomposing problems into intermediate steps before arriving at a final answer.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> [Rest of prompt] Before answering the question, please think about it step-by-step within  `<thinking>``</thinking>`  tags. Then, provide your final answer within  `<answer>`  `</answer>`  tags.\n",
    "\n",
    "### Tree of Thought Prompting (ToT)\n",
    "\n",
    "[Tree-of-Thought (ToT) prompting](https://arxiv.org/abs/2305.10601)  generalizes over Chain-of-Thought by enabling exploration of different aspects (branches) of a problem. This method allows the model to consider multiple reasoning paths, self-evaluate choices, and decide the best course of action. It is particularly useful for complex tasks that require exploring different solutions simultaneously.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> I need help solving a complex task. Please use a Tree of Thoughts approach to break down the task into smaller components, explore different pathways for each component, and provide a well-structured solution. The task is:\n",
    "\n",
    "### Prompt Chaining and Decomposition\n",
    "\n",
    "[Prompt chaining](https://www.promptingguide.ai/techniques/prompt_chaining)  involves breaking down large prompts into a logical flow of smaller prompts linked together by an orchestration layer. This method enhances transparency, control, and reliability in LLM applications. It also helps in maintaining and scaling prompts, reducing latency, and optimizing costs by using smaller, more efficient prompts.\n",
    "\n",
    "**Orchestration Methods:**\n",
    "\n",
    "-   **In Code**: Implementing a semantic router to manage prompt flow.\n",
    "-   **Traditional Tools**: Using Airflow, Step Functions, etc.\n",
    "-   **3rd Party Libraries**: Utilizing LangChain, LlamaIndex, etc.\n",
    "\n",
    "**Example - Prompt Chaining for Document QA:**\n",
    "\n",
    "<u>Prompt 1</u>:\n",
    "\n",
    "> You are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by  `####`. Please output the list of quotes using  `<quotes></quotes>`. Respond with \"No relevant quotes found!\" if no relevant quotes were found.\n",
    "> \n",
    "> #### {{document}}\n",
    "\n",
    "<u>Prompt 2</u>:\n",
    "\n",
    "> Given a set of relevant quotes (delimited by  `<quotes></quotes>`) extracted from a document and the original document (delimited by  `####`), please compose an answer to the question. Ensure that the answer is accurate, has a friendly tone, and sounds helpful.\n",
    "> \n",
    "> #### {{document}}\n",
    "> \n",
    "> `<quotes>`  \n",
    "> ...  \n",
    "> `</quotes>`\n",
    "\n",
    "### Tool Use and Agents - Are These the Same?\n",
    "\n",
    "Tool use and agents, while related, are distinct concepts. Agents dynamically decide which tools to use and in what sequence, making them suitable for open-ended scenarios. Tool use, on the other hand, involves predetermined workflows where the application interacts with other systems (such as text-to-SQL converters, calculators, etc.) in a deterministic manner.\n",
    "\n",
    "**Agents**:\n",
    "\n",
    "-   Operate using the  [ReAct](https://arxiv.org/abs/2210.03629)  approach.\n",
    "-   Suitable for complex, open-ended tasks.\n",
    "-   Can involve multiple steps, higher costs, and increased latency.\n",
    "\n",
    "**Tool Use**:\n",
    "\n",
    "-   Involves semantic routing, where a small model classifies the request and routes it to the appropriate function or tool.\n",
    "-   Suitable for well-defined tasks with clear workflows.\n",
    "\n",
    "**Example Resources:**\n",
    "\n",
    "-   [LangChain](https://www.langchain.com/)  - A framework for developing applications powered by language models.\n",
    "-   [LlamaIndex](https://www.llamaindex.ai/)  - A framework that provides tools for building with LLMs.\n",
    "\n",
    "### Prompt Catalog\n",
    "\n",
    "A prompt catalog is a curated collection of prompts designed to elicit certain behaviors, attributes, or capabilities from a generative AI system.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "-   [OpenAI's Prompt Library](https://platform.openai.com/docs/guides/gpt/prompt-engineering)  - Collection of optimized prompts for a variety of tasks.\n",
    "-   [LangSmith Prompt Hub](https://docs.smith.langchain.com/hub/quickstart)  - Discover, share, and version control prompts for LangChain and LLMs in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71838d45",
   "metadata": {},
   "source": [
    "\n",
    "### Prompt Patterns\n",
    "\n",
    "Prompt patterns, much like software design patterns, offer reusable strategies for crafting effective prompts that guide large language models (LLMs) toward generating desired outputs. As a core element of prompt engineering, these patterns provide a structured approach to solving common challenges encountered during interactions with LLMs, ensuring consistency and reliability in the responses.\n",
    "\n",
    "### Persona Pattern\n",
    "\n",
    "The persona pattern involves defining a fictional or role-based identity for the LLM to adopt when generating responses. This approach provides context and guidelines that shape the model's output, ensuring it aligns with the desired perspective or tone.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When you want the LLM to respond from a specific point of view rather than offering a generic response.\n",
    "-   To maintain consistency in tone, style, or perspective across interactions.\n",
    "-   To engage the model in specialized roles, such as a teacher, advisor, or fictional character.\n",
    "\n",
    "**Prompt Pattern:**  _From now on, act as [persona]. Pay close attention to [details to focus on]. Provide outputs that [persona] would regarding the input._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> You are a history professor with expertise in ancient civilizations. You have a passion for archaeology and enjoy discussing historical mysteries. In your answer, maintain a formal and knowledgeable tone suitable for academic discussions.\n",
    "\n",
    "### How to Implement the Persona Pattern\n",
    "\n",
    "Here's how you can implement the persona pattern using OpenAI's GPT-4:\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "# Define the persona pattern prompt\n",
    "persona_prompt = \"\"\"\n",
    "From now on, act as a history professor with expertise in ancient civilizations. Pay close attention to historical accuracy and depth of insight. Provide outputs that this professor would regarding the input. Maintain a formal and knowledgeable tone suitable for academic discussions.\n",
    "\"\"\"\n",
    "\n",
    "# Example user question\n",
    "user_question = \"Can you explain the significance of the Rosetta Stone in archaeology?\"\n",
    "\n",
    "# Combine persona pattern with the user question\n",
    "prompt = f\"{persona_prompt}\\n\\nUser: {user_question}\\n\"\n",
    "\n",
    "# Send the prompt to GPT-4\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": persona_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output the response\n",
    "print(response['choices'][0]['message']['content'])` \n",
    "```\n",
    "\n",
    "**Expected Outcome:**  The model should respond as a history professor would, providing a detailed, formal explanation about the Rosetta Stone, emphasizing its importance in the field of archaeology and its role in deciphering ancient Egyptian scripts.\n",
    "\n",
    "### Benefits of the Persona Pattern\n",
    "\n",
    "-   **Consistency**: Helps ensure that the responses remain consistent with the specified role or identity throughout the interaction.\n",
    "-   **Specialization**: Allows the model to provide more targeted, informed, and relevant responses by adopting a specialized viewpoint.\n",
    "-   **Engagement**: Can make interactions more engaging and contextually appropriate, especially in educational or advisory settings.\n",
    "\n",
    "**When trying out this pattern**: It's important to provide clear guidelines within the persona prompt to help the model understand the specific tone, style, and perspective it should adopt. The more detailed the persona, the more tailored and relevant the responses will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a83775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_persona:\n",
      "Schrödinger's cat paradox is a thought experiment proposed by physicist Erwin Schrödinger. It presents a cat that could be simultaneously both dead and alive, depending on an earlier random event. This paradox illustrates the problem of the Copenhagen interpretation of quantum mechanics applied to everyday objects, highlighting the conflict between what quantum theory tells us is true about the nature and behavior of matter on the microscopic level and what we observe to be true about the nature and behavior of matter on the macroscopic level.\n",
      "\n",
      "with_persona:\n",
      "Schrödinger's cat paradox is a thought experiment proposed by Erwin Schrödinger, which illustrates the principle of superposition in quantum mechanics. According to this paradox, if a cat is placed in a box with a radioactive atom that might trigger the release of a poison at any moment, the cat can be both alive and dead at the same time until an observer checks, at which point the superposition collapses into one of the possible definite states. It challenges our classical, intuitive understanding of reality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define the question without a persona\n",
    "no_persona = \"\"\"\n",
    "    What is Schrödinger's cat paradox?\n",
    "    Answer concisely.\n",
    "\"\"\"\n",
    "\n",
    "# Define the system message with a persona\n",
    "persona_system = \"From now on, answer as a quantum mechanics philosopher.\"\n",
    "\n",
    "# Messages without persona\n",
    "messages_no_persona = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": no_persona\n",
    "    }\n",
    "]\n",
    "\n",
    "# Messages with persona\n",
    "messages_with_persona = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": persona_system\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": no_persona\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to call GPT-4 and get the completion\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7  # Adjust temperature as needed\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Get and print the response without persona\n",
    "response_no_persona = get_openai_completion(messages_no_persona)\n",
    "print(f'no_persona:\\n{response_no_persona}\\n')\n",
    "\n",
    "# Get and print the response with persona\n",
    "response_with_persona = get_openai_completion(messages_with_persona)\n",
    "print(f'with_persona:\\n{response_with_persona}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ba1a6",
   "metadata": {},
   "source": [
    "### Flipped Interaction Pattern\n",
    "\n",
    "In this pattern, the language model asks questions to the user to gather the necessary information before completing a task or generating the desired output. This approach leverages the model's ability to guide the user through a thought process, ensuring that all necessary details are considered.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When the task requires information you may not have fully considered.\n",
    "-   To leverage the model's knowledge to uncover details you might overlook.\n",
    "-   When the task is complex and requires iterative clarification.\n",
    "\n",
    "**Prompt Pattern:**  _From now on, I would like you to ask me questions to [do a specific task]. When you have enough information to [do the task], create [output you want]._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> I would like you to ask me questions to write a marketing campaign for a new product launch. When you have enough information, list the steps needed for the successful launch of the product.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a3c55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_flip:\n",
      "The best AWS service to handle streaming ingestion is Amazon Kinesis. It is specifically designed to collect, process, and analyze real-time, streaming data so businesses can get timely insights and react quickly to new information. It supports data ingestion from multiple sources and can handle any amount of streaming data and process data from hundreds of thousands of sources with very low latencies.\n",
      "\n",
      "with_flip:\n",
      "1. What is the nature of the data you will be streaming? Is it video, audio, textual data, or something else?\n",
      "2. How much data do you anticipate streaming per day?\n",
      "3. What is the speed or frequency at which you will be streaming this data?\n",
      "4. What are your primary goals from this streaming architecture? For example, real-time analytics, data processing, etc.?\n",
      "5. Are there any specific latency requirements for the data processing?\n",
      "6. How long do you need to store this streaming data?\n",
      "7. Are there any data security or compliance requirements that need to be taken into account?\n",
      "8. Do you have any requirements around data transformation or pre-processing before it’s stored?\n",
      "9. What is your preferred method of data consumption? How will your downstream applications or users consume the data?\n",
      "10. Do you have any scalability concerns or growth projections for the future?\n",
      "11. Do you prefer a fully managed service or are you open to managing some aspects of the service yourself?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define the prompt for the no_flip scenario\n",
    "no_flip = \"What is the best AWS service to handle streaming ingestion?\"\n",
    "\n",
    "# Messages for no_flip scenario\n",
    "messages_no_flip = [\n",
    "    {\"role\": \"user\", \"content\": no_flip}\n",
    "]\n",
    "\n",
    "# Define the prompt for the with_flip scenario\n",
    "with_flip = \"\"\"\n",
    "You are an AWS Solutions Architect with extensive knowledge of AWS analytics services.\n",
    "I would like you to ask me questions to help suggest a streaming architecture for a customer.\n",
    "Once you have gathered enough information, create a concise summary explaining the recommended \n",
    "architecture and its benefits. When ready, ask me the questions and wait for my response.\n",
    "\"\"\"\n",
    "\n",
    "# Messages for with_flip scenario\n",
    "messages_with_flip = [\n",
    "    {\"role\": \"system\", \"content\": with_flip}\n",
    "]\n",
    "\n",
    "# Function to call GPT-4 and get the completion\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7  # Adjust temperature as needed\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Get and print the response for no_flip scenario\n",
    "response_no_flip = get_openai_completion(messages_no_flip)\n",
    "print(f'no_flip:\\n{response_no_flip}\\n')\n",
    "\n",
    "# Get and print the response for with_flip scenario\n",
    "response_with_flip = get_openai_completion(messages_with_flip)\n",
    "print(f'with_flip:\\n{response_with_flip}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa096a2b",
   "metadata": {},
   "source": [
    "### Question Refinement Pattern\n",
    "\n",
    "The Question Refinement Pattern involves asking the LLM to provide a refined or improved version of a question that the user has asked. This is particularly useful when the user's initial question is broad, vague, or lacking in detail, and the LLM can leverage its knowledge to suggest a more precise and effective question.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When the initial question is broad or lacks specificity.\n",
    "-   When the user may not have deep expertise in the field and could benefit from the LLM's insights.\n",
    "-   To improve the quality of responses by starting with a well-crafted question.\n",
    "\n",
    "**Prompt Pattern:**  _From now on, when I ask a question, suggest a better version of the question to use that incorporates information specific to [use case] and ask me if I would like to use your question instead._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> Whenever I ask a question about implementing a feature, suggest a better version of the question that focuses on best practices and the specific programming language or framework I’m using, and ask me if I would like to use your question instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a0017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "<refined>What were the main political, social, and economic factors that triggered the outbreak of the First World War?</refined> Would you like to use this question instead?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define the system message for the question refinement pattern\n",
    "system_message = \"\"\"\n",
    "From now on, when I ask a question, suggest a better version of the question to use that \n",
    "incorporates information specific to the question asked and ask me if I would like \n",
    "to use your question instead. Enclose the new question within <refined> XML tags.\n",
    "\"\"\"\n",
    "\n",
    "# Define the user's prompt\n",
    "user_prompt = \"What caused the first world war?\"\n",
    "\n",
    "# Messages to initiate the interaction\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to call GPT-4 and get the completion\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7  # Adjust temperature as needed\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Get the response where GPT-4 suggests a refined question\n",
    "response = get_openai_completion(messages)\n",
    "print(f'LLM Response:\\n{response}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb3c36",
   "metadata": {},
   "source": [
    "### Cognitive Verifier Pattern\n",
    "\n",
    "The Cognitive Verifier Pattern involves the LLM breaking down a high-level or complex question into several sub-questions. The user answers these sub-questions, and the LLM then combines these answers to generate a final, more accurate response to the original question.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When the task requires a logical sequence or detailed reasoning.\n",
    "-   For organizing narratives, outlining longer content, or handling complex questions.\n",
    "-   When the user may not have deep expertise in the subject matter and could benefit from breaking down the problem.\n",
    "\n",
    "**Prompt Pattern:**  _When I ask you a question, generate three additional questions that would help you give a more accurate answer. When I have answered the three questions, combine the answers to produce the final answer to my original question._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> I ask a question about climate change, break it down into three smaller questions that would help you provide a more accurate answer. Combine the answers to these sub-questions to give the final answer.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccde7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "1. Are you asking about the best soccer player in terms of career achievements or current form?\n",
      "2. Is there a specific league or country you are interested in?\n",
      "3. Are you looking for a subjective opinion or are you asking based on certain statistics or awards?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from src.completions import get_openai_completion\n",
    "\n",
    "# Define the system prompt for the Cognitive Verifier Pattern\n",
    "system_prompt = \"\"\"\n",
    "When I ask you a question, generate three additional questions that would help you \n",
    "give a more accurate answer. When I have answered the three questions, combine the \n",
    "answers to produce the final answer to my original question.\n",
    "\"\"\"\n",
    "\n",
    "# Define the user's initial question\n",
    "user_prompt = \"\"\"\n",
    "Who is considered the best soccer player?\n",
    "\"\"\"\n",
    "\n",
    "# Messages to initiate the interaction\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the response where GPT-4 generates sub-questions\n",
    "response = get_openai_completion(messages)\n",
    "print(f'LLM Response:\\n{response}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee071c8e",
   "metadata": {},
   "source": [
    "### Recipe Pattern\n",
    "\n",
    "The Recipe Pattern is designed to create a structured, step-by-step guide that the LLM can follow to accomplish a specific task or achieve a goal. It is particularly useful when the user has limited knowledge and needs the LLM to provide a comprehensive solution.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When you need a detailed, actionable guide to accomplish a task.\n",
    "-   When the user may not have enough knowledge to fully outline the steps required.\n",
    "-   To ensure that all necessary steps are included, even if the user isn’t aware of them.\n",
    "\n",
    "**Prompt Pattern:**  _I want you to act as [use case]. I want you to provide me a list of suggestions [for the use case]. Your suggestions should be [specific, actionable, etc.]. Do not provide [irrelevant information or a particular detail]._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> I need to deploy a cloud application. Some of the steps require logging into the cloud console, instantiating an instance, and installing dependencies. Please provide a complete sequence of steps. Please fill in any missing steps.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d257ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_recipe:\n",
      "Net profit is calculated by subtracting all your business expenses from your total revenue. This includes costs like operating expenses, taxes, interest and depreciation. \n",
      "\n",
      "Here is a simple formula: \n",
      "\n",
      "Net Profit = Total Revenue – Total Expenses\n",
      "\n",
      "If your expenses are higher than your revenue, you will have a net loss. If your revenue is higher than your expenses, you will have a net profit. \n",
      "\n",
      "Note: It's important to accurately record all your expenses and revenue to get an accurate calculation. You also need to consider both cash and non-cash items. \n",
      "\n",
      "Also, net profit can be different from operating profit, gross profit, or other types of profit. Each type of profit gives you a different perspective on your company's financial health and performance.\n",
      "\n",
      "with_recipe:\n",
      "Step 1: Calculate Total Revenues\n",
      "Total revenues are the total receipts from selling your products or services. This could include sales of goods, return on investments, and any other income your business makes. \n",
      "\n",
      "Step 2: Calculate Cost of Sales\n",
      "Cost of sales, also known as cost of goods sold (COGS), includes all the direct costs attributable to the production of the goods sold in a company. This will include the cost of materials, direct labor costs, and any other direct costs associated with the production of goods.\n",
      "\n",
      "Step 3: Subtract Cost of Sales from Total Revenues\n",
      "Subtracting the cost of sales from your total revenues will give you your gross profit. Gross profit represents your total earnings from sales before administrative, financial, and operational costs are deducted.\n",
      "\n",
      "Step 4: Calculate Operating Expenses\n",
      "Operating expenses are the costs associated with running your business that are not directly tied to the production of goods or services. This could include rent, utilities, salaries for non-production employees, insurance, depreciation, and marketing and advertising costs.\n",
      "\n",
      "Step 5: Subtract Operating Expenses from Gross Profit\n",
      "This will give you your operating profit, also known as EBIT (earnings before interest and taxes). It represents the profit your company made from its operations alone without taking into account interest or tax expenses.\n",
      "\n",
      "Step 6: Calculate Interest and Taxes\n",
      "Interest is the cost of borrowing money, and taxes are based on your company's taxable income. The taxation rate in the USA varies depending on the state and the type of income. For companies, the federal tax rate is 21%. State tax rates vary but are generally between 1% and 12%.\n",
      "\n",
      "Step 7: Subtract Interest and Taxes from Operating Profit\n",
      "This is your net profit, also known as net income or bottom line. It’s the amount of money left over after all costs, expenses, taxes, and interest payments are deducted from the total revenue.\n",
      "\n",
      "These are the fundamental steps that you should follow. There are no unnecessary steps, and none are missing. However, your calculations could be more complicated depending on the specifics of your business and the accounting principles you follow. For instance, you may need to account for extraordinary items, non-operating income, and other factors in your calculation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario without using the Recipe Pattern\n",
    "no_recipe = \"\"\"\n",
    "    How can I calculate the net profit of my company?\n",
    "\"\"\"\n",
    "messages_no_recipe = [\n",
    "    {\"role\": \"user\", \"content\": no_recipe}\n",
    "]\n",
    "\n",
    "# Get the response without using the Recipe Pattern\n",
    "response_no_recipe = get_openai_completion(messages_no_recipe)\n",
    "print(f'no_recipe:\\n{response_no_recipe}\\n')\n",
    "\n",
    "# Scenario using the Recipe Pattern\n",
    "with_recipe = \"\"\"\n",
    "    I am trying to calculate the net profit of my company. \n",
    "    I know that I need to take into account the total revenues, \n",
    "    cost of sales, and taxation in the USA. Provide a complete sequence of steps. \n",
    "    Fill in any missing steps. Identify any unnecessary steps.\n",
    "\"\"\"\n",
    "messages_with_recipe = [\n",
    "    {\"role\": \"user\", \"content\": with_recipe}\n",
    "]\n",
    "\n",
    "# Get the response using the Recipe Pattern\n",
    "response_with_recipe = get_openai_completion(messages_with_recipe)\n",
    "print(f'with_recipe:\\n{response_with_recipe}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d2d33",
   "metadata": {},
   "source": [
    "### Template Pattern\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When you need the LLM to generate a response with a clear and consistent structure.\n",
    "-   To ensure that the response follows a specific format, such as for reports, essays, or structured summaries.\n",
    "\n",
    "**Prompt Pattern:**  _Please generate a response following the given template: [Template]_\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> Please generate a response following the given template:  \n",
    "> **Introduction**  — [Introductory sentence].  \n",
    "> **Main Points**  — [Key points to be covered].  \n",
    "> **Conclusion**  — [Concluding statement].\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd1f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Pattern Response:\n",
      "Dear Marco,\n",
      "\n",
      "Greetings from AnyHotel!\n",
      "As a gift for your 38th birthday, we are happy to recognize 38 Points for your loyalty.\n",
      "\n",
      "Best wishes,\n",
      "AnyHotel Staff\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the template prompt\n",
    "template_prompt = \"\"\"\n",
    "I am going to provide a template for your output. Everything in all caps is a placeholder. \n",
    "Any time that you generate text, try to fit it into one of the placeholders that I list. \n",
    "Please preserve the formatting and overall template that I provide.\n",
    "\n",
    "Write an email for Marco's 38th birthday and gift 38 points:\n",
    "\n",
    "Dear PERSON,\n",
    "\n",
    "Greetings from AnyHotel!\n",
    "As a gift for your birthday, we are happy to recognize NUMBER_OF_POINTS Points for your loyalty.\n",
    "\n",
    "Best wishes,\n",
    "AnyHotel Staff\n",
    "\"\"\"\n",
    "\n",
    "# Messages to initiate the interaction\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": template_prompt}\n",
    "]\n",
    "\n",
    "# Get the response where GPT-4 generates content based on the template\n",
    "response = get_openai_completion(messages)\n",
    "print(f'Template Pattern Response:\\n{response}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a07e1",
   "metadata": {},
   "source": [
    "### Reflection Pattern\n",
    "\n",
    "The Reflection Pattern is useful when the user needs to understand not just the answer provided by the LLM, but also the reasoning, assumptions, and potential limitations behind that answer. This pattern is particularly valuable when the user must judge the quality, validity, or appropriateness of the response, and it allows for a deeper exploration of how the LLM arrives at its conclusions.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When the user needs to evaluate the legitimacy of the LLM's response.\n",
    "-   When understanding the LLM's thought process is important for refining prompts or improving the quality of future interactions.\n",
    "-   When the task involves complex or ambiguous topics that could benefit from a detailed explanation of the reasoning behind the answer.\n",
    "\n",
    "**Prompt Pattern:**  _When you provide an answer, please explain the reasoning and assumptions behind your response. If possible, use specific examples or evidence to support your answer. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> Tell me ideas for a trendy TikTok video; When you provide an answer, please explain the reasoning and assumptions behind your response. If possible, use specific examples or evidence to support why this idea is great for a TikTok video. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91dcb52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_reflection:\n",
      "Yes, in terms of color theory, black is considered the darkest color. It absorbs all colors of light and reflects none, which is why it appears so dark to the human eye.\n",
      "\n",
      "with_reflection:\n",
      "Yes, black is typically considered the darkest color. The perception of color is subjective and can vary depending on the viewer's perception or particular lighting conditions, but in general, black is seen as the darkest because it absorbs all light and reflects none of it back to the eye. This is why objects appear black—they are absorbing all the light that hits them, with no light being reflected back, which would create a sense of color.\n",
      "\n",
      "However, it's important to note a few limitations to this answer. First, color is a human perception and not an inherent property of objects. Different species perceive color differently, and even among humans, there can be variation, especially among those with color vision deficiencies. Second, there are different shades of black, some of which may appear darker than others. In recent years, a material known as Vantablack has been developed that is said to be the darkest substance on earth, absorbing 99.965% of visible light. So while black is typically the darkest color, the exact \"darkest\" may vary depending on the specific material or pigment in question.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the user's prompt without using the reflection pattern\n",
    "no_reflection_prompt = \"\"\"\n",
    "    Is Black the darkest color?\n",
    "\"\"\"\n",
    "\n",
    "messages_no_reflection = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": no_reflection_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the response without using the reflection pattern\n",
    "response_no_reflection = get_openai_completion(messages_no_reflection)\n",
    "print(f'no_reflection:\\n{response_no_reflection}\\n')\n",
    "\n",
    "# Define the system prompt for using the reflection pattern\n",
    "system_reflection = \"\"\"\n",
    "When you provide an answer, please explain the reasoning and assumptions behind your response. \n",
    "If possible, use specific examples or evidence to support your answer. \n",
    "Moreover, please address any potential ambiguities or limitations in your answer, \n",
    "in order to provide a more complete and accurate response.\n",
    "\"\"\"\n",
    "\n",
    "messages_with_reflection = [\n",
    "    {\"role\": \"system\", \"content\": system_reflection},\n",
    "    {\"role\": \"user\", \"content\": no_reflection_prompt}\n",
    "]\n",
    "\n",
    "# Get the response using the reflection pattern\n",
    "response_with_reflection = get_openai_completion(messages_with_reflection)\n",
    "print(f'with_reflection:\\n{response_with_reflection}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7e22e",
   "metadata": {},
   "source": [
    "### Ask for Input Pattern\n",
    "\n",
    "The Ask for Input Pattern is used when you want the LLM to actively seek additional information or clarification from the user in order to complete a task effectively. This pattern is particularly useful in scenarios where the LLM needs more context or specific details to provide an accurate and relevant response.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When the LLM requires more information to proceed with a task.\n",
    "-   In situations where user input is needed to clarify ambiguities or fill in gaps in the initial request.\n",
    "-   To ensure that the LLM's output is tailored to the specific needs and context provided by the user.\n",
    "\n",
    "**Prompt Pattern:**  _From now on, I am going to [describe the task or context]. You will [describe what the LLM should do]. At the end, [describe any final actions the LLM should take]. Ask me for [describe the specific input you want the LLM to request]._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> From now on, I am going to cut/paste email chains into our conversation. You will summarize what each person’s points are in the email chain. You will provide your summary as a series of sequential bullet points. At the end, list any open questions or action items directly addressed to me. My name is Jill Smith. Ask me for the first email chain.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23eddf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response:\n",
      "Could you please specify your location of residence? Your location's climate and soil conditions play a significant role in determining which plant would grow best.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your location:  india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response:\n",
      "India has a diverse climate, ranging from tropical in the south to temperate in the north. However, considering the overall climate, I would suggest growing a Mango tree (Mangifera Indica). Mango trees are native to India and are well-suited to the tropical and subtropical climates found in many parts of the country. They are also widely regarded for their sweet fruit, and their trees are known for their longevity and hardiness.\n",
      "\n",
      "Alternatively, if you're looking for indoor plants, Snake plants (Sansevieria) or Money plants (Pothos) could be good options as they are easy to care for and are known to purify indoor air. Remember, the specific plant choice should also consider your local conditions and your ability to care for the plant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a professional agriculturist agent.\n",
    "Based on the location, you will provide a reasoned suggestion on which plant to buy.\n",
    "If the location is missing, ask the user to provide their location of residence.\n",
    "\"\"\"\n",
    "\n",
    "# Define the user's initial question\n",
    "user_prompt = \"\"\"\n",
    "Which plant should I buy?\n",
    "\"\"\"\n",
    "\n",
    "# Messages to initiate the interaction\n",
    "messages_initial = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the response where GPT-4 might ask for the location\n",
    "response_initial = get_openai_completion(messages_initial)\n",
    "print(f\"Initial Response:\\n{response_initial}\\n\")\n",
    "\n",
    "# Ask the user for the location based on the LLM's request\n",
    "location = input(\"Enter your location: \")\n",
    "\n",
    "# Continue the conversation with the location provided\n",
    "messages_follow_up = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response_initial\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": location\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the final response after providing the location\n",
    "response_final = get_openai_completion(messages_follow_up)\n",
    "print(f\"Final Response:\\n{response_final}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361682f3",
   "metadata": {},
   "source": [
    "### Fact Checklist Pattern\n",
    "\n",
    "The Fact Checklist Pattern is a technique where the LLM not only generates a summary or response but also compiles a list of key facts included in the response. This checklist serves as a quick reference to verify the validity and accuracy of the information provided. It is especially useful when the accuracy of certain facts is crucial to the overall understanding and reliability of the output.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "-   When verifying the validity of a response is critical.\n",
    "-   To quickly identify and cross-check the essential facts in a summary or response.\n",
    "-   When you need to ensure that key points are accurate without analyzing the entire response in detail.\n",
    "\n",
    "**Prompt Pattern:**  _After you generate a [Task] summary, compile a list of the key facts. Insert this fact list at the [position] of the summary. Include the main points that would affect the overall understanding of the [Task]._\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> After you generate a news article summary, compile a list of the key facts. Insert this fact list at the end of the summary. Include the main points that would affect the overall understanding of the news story.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f09d3966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_fact:\n",
      "I'm sorry, but I couldn't find specific data on the global share of CO2 emissions by wildfires in 2020. The share can vary widely by year and region due to the unpredictable nature of wildfires. However, according to Carbon Brief, in the United States, wildfires can contribute as much as 5-10% of yearly CO2 emissions. For more accurate information, I recommend checking resources like the Global Fire Emissions Database or the Intergovernmental Panel on Climate Change.\n",
      "\n",
      "with_fact:\n",
      "Determining the exact share of CO2 emissions by wildfires in 2020 globally is challenging due to variability in data collection methods, differences in wildfire intensity, and the complex nature of carbon cycling. However, according to the Global Fire Emissions Database (GFED), the CO2 emissions from wildfires globally was approximately 7.8 billion metric tons in 2020. Given that the total global CO2 emissions were around 33.1 billion metric tons in 2020 according to Global Carbon Project, this would mean that wildfires contributed to about 23.5% of the total CO2 emissions.\n",
      "\n",
      "Fact Check:\n",
      "1. The Global Fire Emissions Database (GFED) provides data on CO2 emissions from wildfires.\n",
      "2. The Global Carbon Project provides data on total global CO2 emissions.\n",
      "3. The calculation is based on the ratio of CO2 emissions from wildfires to total global CO2 emissions. \n",
      "\n",
      "Please note that these are estimates and the actual figures may vary due to the factors mentioned earlier. \n",
      "\n",
      "Sources: \n",
      "1. Global Fire Emissions Database (GFED)\n",
      "2. Global Carbon Project \n",
      "3. Basic arithmetic calculation to determine the percentage share.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the initial user prompt without using the fact checklist pattern\n",
    "no_fact_prompt = \"\"\"\n",
    "What was the share of CO2 emissions by wildfires in 2020 globally?\n",
    "\"\"\"\n",
    "\n",
    "messages_no_fact = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": no_fact_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the response without the fact checklist\n",
    "response_no_fact = get_openai_completion(messages_no_fact)\n",
    "print(f'no_fact:\\n{response_no_fact}\\n')\n",
    "\n",
    "# Define the system prompt to implement the Fact Checklist Pattern\n",
    "system_fact_check = \"\"\"\n",
    "From now on, when you generate an answer, create a set of facts that the answer depends on that \n",
    "should be fact-checked and list this set of facts at the end of your output. Include fact source \n",
    "contribution reference where possible.\n",
    "\"\"\"\n",
    "\n",
    "# Use the same user question with the fact checklist pattern applied\n",
    "messages_with_fact = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_fact_check\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": no_fact_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the response using the fact checklist pattern\n",
    "response_with_fact = get_openai_completion(messages_with_fact)\n",
    "print(f'with_fact:\\n{response_with_fact}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9a372",
   "metadata": {},
   "source": [
    "## Prompt Challenges\n",
    "\n",
    "Below is a curated set of prompting challenges. Each challenge includes a hints section for you to review in case you need some direction. The hints and suggested solution sections aim to provide possible solutions to the challenge, showcasing best practices and techniques covered in this workshop. There is no single solution for each challenge, and you may find other prompts that also solve the challenges.\n",
    "\n",
    "You may encounter instances where your prompt solves the challenge once but fails to repeat it consistently. We encourage you to try adjusting your prompts to achieve the consistency expected in a production system.\n",
    "\n",
    "**Reminder:**  You may override 'get completion' inference parameters in the function call. Here is the function signature using OpenAI's API:\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "def get_openai_completion(messages, \n",
    "                          model=\"gpt-4\", \n",
    "                          max_tokens=2000, \n",
    "                          temperature=0.7, \n",
    "                          top_p=1.0, \n",
    "                          frequency_penalty=0.0, \n",
    "                          presence_penalty=0.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "```\n",
    "\n",
    "In this game quest, you can leave the temperature and other inference parameters at their default values. However, we encourage you to experiment by adjusting them and observing how they influence the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa80e2d",
   "metadata": {},
   "source": [
    "### Challenge #1\n",
    "\n",
    "**Task:**  \n",
    "Given this prompt: \"What is the capital of France?\" modify it so it will output only a single word: \"Paris.\"\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 \n",
    "\n",
    "**Instructions:**  \n",
    "Try to solve the challenge by crafting the prompt. If you're unsure, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Ask GPT-4 to provide a concise response, or instruct it to \"respond with a single word only.\" <details> <summary>Solution</summary> What is the capital of France? Respond with a single word only. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "By instructing the model to \"respond with a single word only,\" you're guiding it to give a concise answer, which in this case should be \"Paris.\"\n",
    "\n",
    "This challenge helps you practice controlling the verbosity of the model's output, which can be useful in many applications where brevity is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e942ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "# Modify the prompt to instruct the model to provide a single-word answer\n",
    "prompt = \"What is the capital of France? Respond with a single word only.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the completion using GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d11d33",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge #2\n",
    "\n",
    "**Task:**  \n",
    "Write a prompt to solve this word puzzle: Use only eight 8s and addition to get the number 1,000.\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (since we're aligning with OpenAI's models instead of Mistral 7B-Instruct)\n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the puzzle by crafting a prompt. If you're stuck after a few attempts, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Sometimes, solving a task requires using larger models with enhanced reasoning capabilities to verify that the task can indeed be solved. Once you have a working prompt, use prompt engineering to refine the solution. <details> <summary>Solution</summary> The solution to the puzzle is: `888 + 88 + 8 + 8 + 8 = 1000`. A possible prompt could be: \"Use only eight 8s and addition to form the number 1,000. How can this be done?\" </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge is about guiding the model to solve a mathematical puzzle. The solution involves arranging the digits in a specific way, and the hint suggests using a larger, more capable model to find the solution before refining the prompt for a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "026a421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888 + 88 + 8 + 8 + 8 = 1000\n"
     ]
    }
   ],
   "source": [
    "# Define the puzzle prompt\n",
    "prompt = \"Use only eight 8s and addition to get the number 1,000.\"\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "# If you're using GPT-4, you should call the appropriate function\n",
    "response = get_openai_completion(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc2cc",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge #3\n",
    "\n",
    "**Task:**  \n",
    "Write a prompt to solve this question correctly:\n",
    "\n",
    "> A bakery sells muffins and cookies. On Monday, the bakery sold 30 items and made $120 in revenue. Muffins cost $3 each, and cookies cost $2 each. The bakery sold twice as many muffins as cookies. How many muffins and cookies did the bakery sell on Monday?\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (If aligning with OpenAI) or Mistral 7B-Instruct (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:**  \n",
    "Try to solve the challenge by crafting a prompt. If you're stuck, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Since this requires sequential reasoning, try using the Chain of Thought technique and ground the reasoning process using the Recipe Pattern. <details> <summary>Solution</summary> Start with the following prompt: \"A bakery sells muffins and cookies. On Monday, the bakery sold 30 items and made $120 in revenue. Muffins cost $3 each, and cookies cost $2 each. The bakery sold twice as many muffins as cookies. How many muffins and cookies did the bakery sell on Monday? When answering, start by providing a complete sequence of steps first, then solve step by step.\" </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves solving a word problem that requires logical reasoning and a step-by-step approach. The hint suggests using the Chain of Thought technique, which involves breaking down the problem into smaller, manageable steps, and using the Recipe Pattern to organize the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2633a1bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_mistral_completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m format_instructions([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Get the completion using the Mistral model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m get_mistral_completion(formatted_prompt)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_mistral_completion' is not defined"
     ]
    }
   ],
   "source": [
    "# Modify the prompt, if required, and run\n",
    "prompt = \"\"\"\n",
    "A bakery sells muffins and cookies. On Monday, the bakery sold 30 items and made $120 in revenue.\n",
    "Muffins cost $3 each, and cookies cost $2 each. The bakery sold twice as many muffins as cookies.\n",
    "How many muffins and cookies did the bakery sell on Monday?\n",
    "\"\"\"\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_mistral_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07197cd0-ace7-43cf-a3a5-884c1eeb27a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:\n",
      "1. Write down the equations based on the given information.\n",
      "2. Solve the equations simultaneously.\n",
      "\n",
      "Let's denote the number of muffins as m and the number of cookies as c.\n",
      "\n",
      "From the problem, we have two equations:\n",
      "\n",
      "1. The bakery sold 30 items in total, so m + c = 30.\n",
      "2. The bakery sold twice as many muffins as cookies, so m = 2c.\n",
      "3. The total revenue was $120, so 3m + 2c = 120.\n",
      "\n",
      "Step 1:\n",
      "Replace m in the first equation with 2c from the second equation:\n",
      "\n",
      "2c + c = 30.\n",
      "\n",
      "Solve it for c:\n",
      "\n",
      "3c = 30,\n",
      "c = 30 / 3,\n",
      "c = 10.\n",
      "\n",
      "Step 2:\n",
      "Insert c = 10 back into the second equation to find m:\n",
      "\n",
      "m = 2*10,\n",
      "m = 20.\n",
      "\n",
      "So, the bakery sold 20 muffins and 10 cookies on Monday.\n"
     ]
    }
   ],
   "source": [
    "# Define the word problem prompt using the Recipe Pattern\n",
    "prompt = \"\"\"\n",
    "A bakery sells muffins and cookies. On Monday, the bakery sold 30 items and made $120 in revenue.\n",
    "Muffins cost $3 each, and cookies cost $2 each. The bakery sold twice as many muffins as cookies.\n",
    "How many muffins and cookies did the bakery sell on Monday?\n",
    "\n",
    "When answering, start by providing a complete sequence of steps first, then solve step by step.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "# Get the completion using GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396feb7",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge #4\n",
    "\n",
    "**Task:**  \n",
    "Write a prompt that instructs the model to engage in a conversational interaction with you to determine the most suitable AWS service for capturing and processing large volumes of stream events in near real-time.\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (If aligning with OpenAI) or Claude 3 Haiku (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:**  \n",
    "Try to solve the challenge by crafting a prompt. If you're unsure, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Use the Flipped Interaction pattern to guide the model into asking questions that help determine the best solution. <details> <summary>Solution</summary> I need your help in selecting the most appropriate AWS service for capturing and processing large volumes of stream events in near real-time. Please engage in a conversational interaction with me, asking questions that will help you choose the most appropriate solution. Gather the necessary information about the use case and my requirements. Once you have enough details, recommend the optimal AWS service(s) and explain your reasoning. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves guiding the model to engage in a dynamic, interactive conversation where it asks questions to gather the necessary information to make a recommendation. The Flipped Interaction pattern is particularly useful here as it encourages the model to lead the conversation by asking relevant questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb246ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "prompt = \"\"\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "# Get the completion using GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837ae78",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge #5\n",
    "\n",
    "**Task:**  \n",
    "Modify the prompt to correctly answer: Is 'x' in the equation below solved correctly?\n",
    "\n",
    "Given equation:\n",
    "\n",
    "```\n",
    "2x - 3 = 9  \n",
    "2x = 6  \n",
    "x = 3 \n",
    "```\n",
    "**Model to use:**  \n",
    "GPT-4 (If aligning with OpenAI) or Claude 3 Haiku (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:** \n",
    "Try solving the challenge by crafting a prompt. If you're stuck, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Ask the model to review the solution steps carefully, one by one. <details> <summary>Solution</summary> Review the solution steps one by one. Is 'x' in the equation below solved correctly? 2x - 3 = 9 2x = 6 x = 3 </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves verifying whether the steps to solve for 'x' in the given equation are correct. The solution requires the model to carefully check each step to determine if the final solution for 'x' is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06e8eec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, the equation is not solved correctly. The correct steps should be:\n",
      "\n",
      "Start with the original equation:\n",
      "2x - 3 = 9\n",
      "\n",
      "Add 3 to both sides to isolate 2x on one side of the equation:\n",
      "2x = 12\n",
      "\n",
      "Finally, divide both sides by 2 to solve for x:\n",
      "x = 6\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt asking the model to review the solution steps\n",
    "prompt = \"\"\"\n",
    "Review the solution steps one by one. Is 'x' in the equation below solved correctly?\n",
    "\n",
    "2x - 3 = 9  \n",
    "2x = 6  \n",
    "x = 3\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "# Get the completion using GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e2c3f",
   "metadata": {},
   "source": [
    "### Challenge #6\n",
    "\n",
    "**Task:**  \n",
    "Write a prompt that calculates the result of:  `1984135 * 9343116`.\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (If aligning with OpenAI) or Claude 3 Haiku (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the challenge by crafting a prompt. If you're stuck after several attempts, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> As of now, LLMs have limitations in performing complex numerical computations accurately. LLMs are better suited for understanding and generating human-like text, but they aren't optimized for performing large-scale numerical calculations. Consider using external tools or APIs designed for accurate mathematical computations. <details> <summary>Solution</summary> Instead of relying on an LLM, use tools or APIs that are designed to perform precise mathematical operations. For example, you could use a calculator, Python, or any other programming language to compute the result. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "Language models like GPT-4 or Claude are powerful for understanding and generating text, but they have limitations when it comes to performing complex numerical calculations, especially involving large numbers. The hint suggests using tools specifically designed for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "prompt = \"\"\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "# Get the completion using GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e746bc",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge #7\n",
    "\n",
    "**Task:**  \n",
    "Write a tool-use prompt that assesses the question and decides which math function(s) to call, and with what arguments. Test your solution on: \"Please solve 1984135 * 9343116\".\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (If aligning with OpenAI)\n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the challenge by crafting a prompt that instructs the model to assess the question, identify the necessary mathematical operations, and suggest the appropriate function calls.\n",
    "\n",
    "<details> <summary>Hint</summary> Review the section on tool use from the Claude workshop, which discusses how to create prompts that enable the model to select and execute functions. <details> <summary>Solution</summary> The solution involves creating a prompt that instructs the model to identify that multiplication is needed and then calls the appropriate function with the correct arguments. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves designing a prompt that can instruct the model to recognize the need for a mathematical operation (in this case, multiplication) and to call the relevant function with the correct parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0f830-7eba-442d-a313-663fd092cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that could be invoked based on the model's response\n",
    "def mul(a, b):\n",
    "    return a * b\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def sub(a, b):\n",
    "    return a - b\n",
    "\n",
    "# System message to guide the assistant\n",
    "system_message = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# User's prompt\n",
    "user_prompt = \"Please solve 1984135 * 9343116\"\n",
    "\n",
    "# Messages to guide the conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the response from GPT-4\n",
    "response = get_openai_completion(messages)\n",
    "print(f\"Model's response:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07087567",
   "metadata": {},
   "source": [
    "### Challenge #8\n",
    "\n",
    "**Task:**  \n",
    "Write a prompt to extract key business trends from the image  `assets/tabular-data.png`, and convert the data to a JSON representation for downstream processing.\n",
    "![assets/tabular-data.png](assets/tabular-data.png) \n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (if aligning with OpenAI) or Claude 3 Haiku (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the challenge by crafting a prompt that guides the model to analyze the data, extract trends, and convert the data into JSON format. If you're stuck, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Be direct, and consider using the Reflection Pattern to understand the model's reasoning process. <details> <summary>Solution</summary> Analyze the tabular data within the image and extract business trends. When you provide an answer, please explain the reasoning and assumptions behind your response. If possible, use specific examples or evidence to support your answer. Moreover, please address any potential ambiguities or limitations in your answer, to provide a more complete and accurate response. Provide your analysis within `<analysis>` XML tags. Finally, export the tabular data to JSON format, and place it within `<json>` XML tags. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves guiding the model to analyze the data in the image, extract business trends, and then convert that data into a JSON format for further processing. Using the Reflection Pattern will allow the model to explain its reasoning, providing transparency and enabling you to judge the validity of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64339a7-b96b-4a7a-b6f7-706460ff90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "\n",
    "# Function that accepts an image file location and returns the base64 representation of the image\n",
    "def get_base64_image(image_path):\n",
    "    \"\"\"\n",
    "    Converts an image file to its base64 representation.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The file path of the image.\n",
    "        \n",
    "    Returns:\n",
    "        str: The base64 representation of the image.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read())\n",
    "    return encoded_string.decode('utf-8')\n",
    "\n",
    "# Define your prompt here\n",
    "prompt = \"\"\"\n",
    "<your_prompt_here>\n",
    "\"\"\"\n",
    "\n",
    "# Define the messages structure, including the image data and the prompt\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": {\n",
    "            \"image\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/jpeg\",\n",
    "                \"data\": get_base64_image(\"assets/tabular-data.png\")\n",
    "            },\n",
    "            \"text\": prompt\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to get the completion using OpenAI's API\n",
    "def get_openai_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        max_tokens=1000,  # Adjust as necessary\n",
    "        temperature=0.7,\n",
    "        top_p=1.0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Call the function to get the completion\n",
    "response = get_openai_completion(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8702e48",
   "metadata": {},
   "source": [
    "### Challenge #9\n",
    "\n",
    "**Task:**  \n",
    "The prompt  `I am trying to market this product, help me think of an advertisement script on social media`  results in a refusal from Haiku. Revise the prompt and make Haiku draft a social media post marketing the product shown in  `assets/mite-and-insect.png`.\n",
    "\n",
    "![Product](assets/mite-and-insect.png)  \n",
    "\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (if aligning with OpenAI) or Claude 3 Haiku (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:**  \n",
    "Try revising the prompt to avoid refusal from the model and successfully generate a social media marketing post for the product. If you're unsure, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Use a user role and add more specific context for the task. <details> <summary>Solution Step 1</summary> Set up a system message like: \"You are a marketing assistant named Joe, working for AnyCompany. The company manufactures and sells products across industries including materials, agriculture, and manufacturing. Your goal is to author social media posts to market the company products. Please respond to the user’s question within `<response></response>` tags.\" </details> <details> <summary>Solution Step 2</summary> Add a prefilled response to guide the model: ```python { \"role\": \"assistant\", \"content\": \"[Joe from AnyCompany] <response>\" } ``` </details> <details> <summary>Note</summary> You might want to compare the results when using system prompts versus embedding the same instructions directly in the prompt text. Evaluate which approach leads to better results. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves crafting a prompt that provides enough context and role guidance to help the model generate an appropriate social media post. By specifying the assistant’s role and company background, you're guiding the model to produce more focused and relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e6ba1-188d-4f1b-9fbf-4016a5872928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System prompt for guiding the assistant (stub, can be customized as needed)\n",
    "system = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# User prompt that includes the task description\n",
    "prompt = \"I am trying to market this product, help me think of an advertisement script on social media\"\n",
    "\n",
    "# Setting up the message structure\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/jpeg\",\n",
    "                    \"data\": get_base64_image(\"assets/mite-and-insect.png\")\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Running the completion (the system message is optional and can be customized)\n",
    "response = get_openai_completion(messages, system=system)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d8348",
   "metadata": {},
   "source": [
    "### Challenge #10\n",
    "\n",
    "**Task:**  \n",
    "Use an LLM to classify whether the following statement is true or false. Verify the response's validity.\n",
    "\n",
    "> If two charged objects are placed in an isolated system and one object loses 5 Coulombs of charge, the other object gains exactly 5 Coulombs of charge.\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (if aligning with OpenAI) \n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the challenge by crafting a prompt that helps the model classify the statement and verify the response using the Fact Checklist Pattern. If you're unsure, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> Use the Fact Checklist Pattern to ensure that the model not only provides an answer but also lists the key facts that support the classification. <details> <summary>Solution</summary> Use a system message like: \"Classify the given statement as either true or false. After generating your answer, compile a list of key facts on which you base your answer.\" </details> <details> <summary>Consideration point</summary> Try the solution prompt with a different model, like Mistral 7B Instruct, and compare the results. You might find that certain models, such as Mistral, are not well-suited for this type of task due to potential gaps in training data. </details> </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves using the Fact Checklist Pattern to ensure that the model's response is not only accurate but also well-supported by key facts. This pattern helps in validating the response, particularly in cases where factual accuracy is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f03af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "system = \"\"\n",
    "prompt = \"\"\"\n",
    "If two charged objects are placed in an isolated system and one object loses 5 Coulombs of charge,\n",
    "the other object gain exactly 5 Coulombs of charge.\n",
    "\"\"\"\n",
    "\n",
    "# Messages to guide the conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Running the completion (the system message is optional and can be customized)\n",
    "response = get_openai_completion(messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06451e",
   "metadata": {},
   "source": [
    "### Challenge #11\n",
    "\n",
    "**Task:**  \n",
    "Make Claude 3 Haiku answer this reasoning task correctly (Jane is wearing blue pants):\n",
    "\n",
    "> Sara, Mary, and Jane are at a party. Sara is wearing red pants, and the person wearing green pants is standing next to the person in blue pants. Jane is standing next to Sara. What color pants is Jane wearing?\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (if aligning with OpenAI)\n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the challenge by crafting a prompt that helps the model reason through the problem correctly. If you're unsure, expand the hint.\n",
    "\n",
    "<details> <summary>Hint 1</summary> Sometimes, after trying different prompts, you might be uncertain whether the model can actually perform the task correctly. It's usually advised to start with the most capable model, as it will require less prompt engineering effort. Once you validate that a model can perform your task, you can work on refining the prompt for a smaller model. <summary>Hint 2</summary> Use the Chain of Thought prompting technique to understand the model's reasoning process. Alternatively, use the Recipe Pattern by adding 'Think step by step and provide a complete sequence of steps.' to the system prompt. Extract the reasoning steps and apply them to the smaller model. </details>\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This challenge involves guiding the model to correctly reason through a scenario where it needs to deduce the color of Jane's pants based on given clues. By using techniques like Chain of Thought or Recipe Pattern, you can help the model think through the problem step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d770e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "system = \"\"\n",
    "prompt = \"\"\"\n",
    "    Sara, Mary, and Jane are at a party. Sara is wearing red pants,\n",
    "    and the person wearing green pants is standing next to the person in blue pants.\n",
    "    Jane is standing next to Sara. What color pants is Jane wearing?\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Messages to guide the conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Running the completion (the system message is optional and can be customized)\n",
    "response = get_openai_completion(messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2885f",
   "metadata": {},
   "source": [
    "### Challenge #12\n",
    "\n",
    "**Task:**  \n",
    "Write a prompt to solve the game of 24 with the numbers 5, 5, 3, and 4. The objective is to make the number 24 using all four numbers exactly once. You can add, subtract, multiply, and divide, but you must use all four numbers only once.\n",
    "\n",
    "**Model to use:**  \n",
    "GPT-4 (if aligning with OpenAI) or Claude 3 Sonnet (as mentioned in the challenge)\n",
    "\n",
    "**Instructions:**  \n",
    "Try solving the challenge by crafting a prompt that guides the model to use a Tree of Thoughts (ToT) approach to break down the problem and solve it step by step. If you're unsure, expand the hint.\n",
    "\n",
    "<details> <summary>Hint</summary> This task will require a Tree-of-Thoughts (ToT) approach, a one-shot example, and careful splitting between the system prompt and messages. Ask Sonnet to solve using ToT, and use the provided example. <details> <summary>Example</summary> Use the example format: ```xml <example> solution for list of numbers 4,5,6,10: 10 - 4 = 6 (left: 5 6 6) 5 * 6 = 30 (left: 6 30) 30 - 6 = 24 (left: 24) Answer: (5 * (10 - 4)) - 6 = 24 </example> ``` </details> <details> <summary>Solution</summary> ```python system = \"\"\" The game of 24 objective: Make the number 24 from a given list of four numbers. You can add, subtract, multiply, and divide. You must use all four listed numbers, but use each number only once. You do not have to use all four operations. Carefully review the given example within the <example> xml tags. \"\"\"\n",
    "\n",
    "```\n",
    "example = \"\"\"\n",
    "    <example>\n",
    "    solution for list of numbers 4,5,6,10:\n",
    "    10 - 4 = 6 (left: 5 6 6)\n",
    "    5 * 6 = 30 (left: 6 30)\n",
    "    30 - 6 = 24 (left: 24)\n",
    "    Answer: (5 * (10 - 4)) - 6 = 24\n",
    "    </example>\n",
    "    \"\"\"\n",
    "\n",
    "    problem = \"\"\"\n",
    "    Solve for the numbers 5, 5, 3, 4 using a Tree of Thoughts approach.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"assistant\", \"content\": example},\n",
    "        {\"role\": \"user\", \"content\": problem}\n",
    "    ]\n",
    "    ```\n",
    "</details> \n",
    "\n",
    "```\n",
    "### Explanation:\n",
    "\n",
    "This challenge requires breaking down the problem into manageable steps and exploring different pathways to reach the solution. By using the Tree of Thoughts (ToT) approach, the model can consider various operations and their outcomes before arriving at the final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c25a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "system = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Evaluate if given numbers can reach 24: 5,5,3,4\n",
    "\"\"\"\n",
    "# Messages to guide the conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Running the completion (the system message is optional and can be customized)\n",
    "response = get_openai_completion(messages, system=system)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
